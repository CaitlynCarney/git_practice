{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan - Acquire - Prepare - Explore - Model - Deliver\n",
    "\n",
    "**Goal**: Prepare, tidy, and clean the data so that it is ready for exploration and analysis.\n",
    "\n",
    "**Input**: 1 or more dataframes acquired through the \"acquire\" step.\n",
    "\n",
    "**Output**: 1 dataset split into 3 samples in the form of dataframes: train, validate & test.\n",
    "\n",
    "**Artifact**: prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How?\n",
    "1. Summarize our data:\n",
    "    - head(), describe(), info(), isnull(), value_counts(), shape, ...\n",
    "    - plt.hist(), plt.boxplot()\n",
    "    - document takeaways (nulls, datatypes to change, outliers, ideas for features, etc.)\n",
    "\n",
    "2. Clean the data:\n",
    "    - missing values: drop columns with too many missing values, drop rows with too many missing values, fill with zero where it makes sense, and then make note of any columns you want to impute missing values in (you will need to do that on split data).\n",
    "    - **outlier**: an observation point that is distant from other observations https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/\n",
    "    - outliers: ignore, drop rows, snap to a selected max/min value, create bins (cut, qcut)\n",
    "    - data errors: drop the rows/observations with the errors, correct them to what it was intended\n",
    "    - address text normalization issues...e.g. deck 'C' 'c'. (correct and standardize the text)\n",
    "    - tidy data: getting your data in the shape it needs to be for modeling and exploring. every row should be an observation and every column should be a feature/attribute/variable. You want 1 observation per row, and 1 row per observation. If you want to predict a customer churn, each row should be a customer and each customer should be on only 1 row. (address duplicates, aggregate, melt, reshape, ...)\n",
    "    - creating new variables out of existing variables (e.g. z = x - y)\n",
    "    - rename columns\n",
    "    - datatypes: need numeric data to be able to feed into model (dummy vars, factor vars, manual encoding)\n",
    "    - scale numeric data: so that continuous variables have the same weight, are on the same units, if algorithm will be used that will be affected by the differing weights, or if data needs to be scaled to a gaussian/normal distribution for statistical testing. (linear scalers and non-linear scalers)\n",
    "\n",
    "3. Split the data:\n",
    "    - split our data into train, validate and test sample dataframes\n",
    "    - Why? overfitting: model is not generalizable. It fits the data you've trained it on \"too well\". 3 points does not necessarily mean a parabola.\n",
    "    - **train**: in-sample, explore, impute mean, scale numeric data (max() - min()...), fit our ml algorithms, test our models.\n",
    "    - **validate, test**: represents future, unseen data\n",
    "    - **validate**: confirm our top models have not overfit, test our top n models on unseen data. Using validate performance results, we pick the top 1 model.\n",
    "    - **test**: out-of-sample, how we expect our top model to perform in production, on unseen data in the future. ONLY USED ON 1 MODEL.\n",
    "    - You want to do all the prep that can be done on the full dataset before you split. Go through, work on DF for all you need to, then move to train when it's time. So you don't have to go back and forth, because leads to errors and inconsistencies in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm vs. model\n",
    "\n",
    "- **algorithm**: the method that sklearn provides, such as decision_tree, knn, ..., y = mx+b\n",
    "- **model**: that algorithm specific to our data, e.g. \n",
    "    - **regression**: the model would contain the slope value and intercept value. y = .2x+5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Should I do this on the full dataset or on the train sample?**\n",
    "\n",
    "this: the action, method, function, step you are about to take on your data.\n",
    "\n",
    "1. Are you comparing, looking at the relationship or summary stats or visualizations with 2+ variables?\n",
    "2. Are you using an sklearn method?\n",
    "3. Are you moving into the explore stage of the pipeline?\n",
    "\n",
    "If **ONE** or more of these is yes, then you should be doing it on your train sample. If **ALL** are no, then the entire dataset is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'warning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0755cefa7198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warning'"
     ]
    }
   ],
   "source": [
    "import warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'acquire' has no attribute 'df_titanic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-003eec250441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#grab our acquired dataset:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_titanic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'acquire' has no attribute 'df_titanic'"
     ]
    }
   ],
   "source": [
    "#grab our acquired dataset:\n",
    "df = acquire.df_titanic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the funciton we defined in the last lesson to acquire our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'acquire' has no attribute 'df_titanic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5bd892d7d19a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_titanic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'acquire' has no attribute 'df_titanic'"
     ]
    }
   ],
   "source": [
    "df = acquire.df_titanic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.columns[[df[col].dtype == 'int64' for col in df.columns]]\n",
    "for col in num_cols:\n",
    "    plt.hist(df[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = df.columns[[df[col].dtype == 'O' for col in df.columns]]\n",
    "for col in obj_cols:\n",
    "    print(df[col].value_counts())\n",
    "    print(df[col].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort = false will sort the bin values as opposed to the frequency counts\n",
    "# value counts of fare by binning\n",
    "df.fare.value_counts(bins=5, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with missing values\n",
    "missing = df.isnull().sum()\n",
    "missing[missing > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways\n",
    "\n",
    "embarked == embark_town, so remove embarked & keep embark_town\n",
    "class == pclass, so remove class & keep pclass (already numeric)\n",
    "drop deck...way too many missing values\n",
    "fill embark_town with most common value ('Southampton')\n",
    "drop age column\n",
    "encode or create dummy vars for sex & embark_town."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates...run just in case\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with too many missing to have any value right now\n",
    "cols_to_drop = ['deck', 'embarked', 'class', 'age']\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could fill embark_town with most common value, 'Southampton', by hard-coding the value using the fillna() function, as below. Or we could use an imputer. We will demonstrate the imputer after the train-validate-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embark_town'] = df.embark_town.fillna(value='Southampton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dummy vars for sex and embark_town\n",
    "\n",
    "dummy_na: create a dummy var for na values, also?\n",
    "drop_first: drop first dummy var (since we know if they do not belong to any of the vars listed, then they must belong to the first one that is not listed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df[['sex','embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "\n",
    "# append dummy df cols to the original df. \n",
    "df = pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to perform these steps when we need to reproduce our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    '''\n",
    "    This function will drop any duplicate observations, \n",
    "    drop columns not needed, fill missing embarktown with 'Southampton'\n",
    "    and create dummy vars of sex and embark_town. \n",
    "    '''\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.drop(columns=['deck', 'embarked', 'class', 'age'], inplace=True)\n",
    "    df.embark_town.fillna(value='Southampton', inplace=True)\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'embark_town']], drop_first=True)\n",
    "    return pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% test, 80% train_validate\n",
    "# then of the 80% train_validate: 30% validate, 70% train. \n",
    "\n",
    "train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.survived)\n",
    "train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option for Missing Values: Impute\n",
    "We can impute values using the mean, median, mode (most frequent), or a constant value. We will use sklearn.imputer.SimpleImputer to do this.\n",
    "\n",
    "Create the imputer object, selecting the strategy used to impute (mean, median or mode (strategy = 'most_frequent').\n",
    "Fit to train. This means compute the mean, median, or most_frequent (i.e. mode) for each of the columns that will be imputed. Store that value in the imputer object.\n",
    "Transform train: fill missing values in train dataset with that value identified\n",
    "Transform test: fill missing values with that value identified\n",
    "\n",
    "Create the SimpleImputer object, which we will store in the variable imputer. In the creation of the object, we will specify the strategy to use (mean, median, most_frequent). Essentially, this is creating the instructions and assigning them to a variable we will reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
