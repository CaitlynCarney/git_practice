{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\" Jason Brownlee, Machine Learning Mastery\n",
    "- https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "- not making features to just make new features but trying to find the ground level.\n",
    "\n",
    "You can construct new features out of existing features, select features the best features, remove the worst features, penalize features by giving them no weight in the model, transform features, as examples.\n",
    "    - been doing this manually\n",
    "    - but there are a lot more tools\n",
    "\n",
    "Some feature engineering methods include:\n",
    "- Construct new features: Use domain knowledge, creating products of features, etc.\n",
    "- Use statistical tests to determine each feature's usefulness in predicting the target variable. Rank the features and then select the K best features (Select K Best).\n",
    "- Recursively remove attributes to meet the number of required features and then build a model on those attributes that remain to see if you can you match or improve performance with a smaller subset (Recursive Feature Elimination).\n",
    "    - will build a whole bunch of model\n",
    "- Recursively remove the worst performing features one by one till the overall performance of the model comes in acceptable range (Backward Elimination).\n",
    "    - random elimination\n",
    "- Add features one at a time beginning with the predictor with the highest correlation with the dependent variable. Variables of greater theoretical importance are entered first. Once in the equation, the variable remains there (Forward Selection).\n",
    "    - building up from less features to more\n",
    "- many, many more...(see appendix for more such as PCA and regularization).\n",
    "    - not enough time right now but you can look uup oand read up more on them.\n",
    "\n",
    "**For the feature engineering methods with regression, we want to use the scaled data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Short Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import env\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test(df, target):\n",
    "    '''\n",
    "    this function takes in a dataframe and splits it into 3 samples, \n",
    "    a test, which is 20% of the entire dataframe, \n",
    "    a validate, which is 24% of the entire dataframe,\n",
    "    and a train, which is 56% of the entire dataframe. \n",
    "    It then splits each of the 3 samples into a dataframe with independent variables\n",
    "    and a series with the dependent, or target variable. \n",
    "    The function returns 3 dataframes and 3 series:\n",
    "    X_train (df) & y_train (series), X_validate & y_validate, X_test & y_test. \n",
    "    '''\n",
    "    # split df into test (20%) and train_validate (80%)\n",
    "    train_validate, test = train_test_split(df, test_size=.2, random_state=123)\n",
    "\n",
    "    # split train_validate off into train (70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, random_state=123)\n",
    "\n",
    "        \n",
    "    # split train into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_train = train.drop(columns=[target])\n",
    "    y_train = train[target]\n",
    "    \n",
    "    # split validate into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_validate = validate.drop(columns=[target])\n",
    "    y_validate = validate[target]\n",
    "    \n",
    "    # split test into X (dataframe, drop target) & y (series, keep target only)\n",
    "    X_test = test.drop(columns=[target])\n",
    "    y_test = test[target]\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "def get_numeric_X_cols(X_train, object_cols):\n",
    "    '''\n",
    "    takes in a dataframe and list of object column names\n",
    "    and returns a list of all other columns names, the non-objects. \n",
    "    '''\n",
    "    numeric_cols = [col for col in X_train.columns.values if col not in object_cols]\n",
    "    \n",
    "    return numeric_cols\n",
    "\n",
    "\n",
    "def min_max_scale(X_train, X_validate, X_test, numeric_cols):\n",
    "    '''\n",
    "    this function takes in 3 dataframes with the same columns, \n",
    "    a list of numeric column names (because the scaler can only work with numeric columns),\n",
    "    and fits a min-max scaler to the first dataframe and transforms all\n",
    "    3 dataframes using that scaler. \n",
    "    it returns 3 dataframes with the same column names and scaled values. \n",
    "    '''\n",
    "    # create the scaler object and fit it to X_train (i.e. identify min and max)\n",
    "    # if copy = false, inplace row normalization happens and avoids a copy (if the input is already a numpy array).\n",
    "\n",
    "\n",
    "    scaler = MinMaxScaler(copy=True).fit(X_train[numeric_cols])\n",
    "\n",
    "    #scale X_train, X_validate, X_test using the mins and maxes stored in the scaler derived from X_train. \n",
    "    # \n",
    "    X_train_scaled_array = scaler.transform(X_train[numeric_cols])\n",
    "    X_validate_scaled_array = scaler.transform(X_validate[numeric_cols])\n",
    "    X_test_scaled_array = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "    # convert arrays to dataframes\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled_array, \n",
    "                                  columns=numeric_cols).\\\n",
    "                                  set_index([X_train.index.values])\n",
    "\n",
    "    X_validate_scaled = pd.DataFrame(X_validate_scaled_array, \n",
    "                                     columns=numeric_cols).\\\n",
    "                                     set_index([X_validate.index.values])\n",
    "\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled_array, \n",
    "                                 columns=numeric_cols).\\\n",
    "                                 set_index([X_test.index.values])\n",
    "\n",
    "    \n",
    "    return X_train_scaled, X_validate_scaled, X_test_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df, object_cols):\n",
    "    '''\n",
    "    This function takes in a dataframe and list of object column names,\n",
    "    and creates dummy variables of each of those columns. \n",
    "    It then appends the dummy variables to the original dataframe. \n",
    "    It returns the original df with the appended dummy variables. \n",
    "    '''\n",
    "    \n",
    "    # run pd.get_dummies() to create dummy vars for the object columns. \n",
    "    # we will drop the column representing the first unique value of each variable\n",
    "    # we will opt to not create na columns for each variable with missing values \n",
    "    # (all missing values have been removed.)\n",
    "    dummy_df = pd.get_dummies(object_cols, dummy_na=False, drop_first=True)\n",
    "    \n",
    "    # concatenate the dataframe with dummies to our original dataframe\n",
    "    # via column (axis=1)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_cols(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and identifies the columns that are object types\n",
    "    and returns a list of those column names. \n",
    "    '''\n",
    "    # create a mask of columns whether they are object type or not\n",
    "    mask = np.array(df.dtypes == \"object\")\n",
    "\n",
    "        \n",
    "    # get a list of the column names that are objects (from the mask)\n",
    "    object_cols = df.iloc[:, mask].columns.tolist()\n",
    "    \n",
    "    return object_cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_student_math(path):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    \n",
    "    # drop any nulls\n",
    "    df = df[~df.isnull()]\n",
    "\n",
    "    # get object column names\n",
    "    object_cols = get_object_cols(df)\n",
    "    \n",
    "    # create dummy vars\n",
    "    df = create_dummies(df, object_cols)\n",
    "      \n",
    "    # split data \n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test(df, 'G3')\n",
    "    \n",
    "    # get numeric column names\n",
    "    numeric_cols = get_numeric_X_cols(X_train, object_cols)\n",
    "\n",
    "    # scale data \n",
    "    X_train_scaled, X_validate_scaled, X_test_scaled = min_max_scale(X_train, X_validate, X_test, numeric_cols)\n",
    "    \n",
    "    return df, X_train, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_scaled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path and stuff\n",
    "path = \"https://gist.githubusercontent.com/ryanorsinger/55ccfd2f7820af169baea5aad3a9c60d/raw/da6c5a33307ed7ee207bd119d3361062a1d1c07e/student-mat.csv\"\n",
    "\n",
    "df, X_train_explore, \\\n",
    "    X_train_scaled, y_train, \\\n",
    "    X_validate_scaled, y_validate, \\\n",
    "    X_test_scaled, y_test = wrangle_student_math(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping nulls on scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest\n",
    "Select the K best features using a statistical test to compare each X with y and find which X's have the strongest relationship with y. For regression, we will use the correlation test (`f-regression`) to score the relationships.\n",
    "\n",
    "1. Initialize the f_selector object, setting the parameters, or instructions for the method to follow: \"use the f_regression test for scoring the features, and return to me the top 10 features\", for example.\n",
    "2. Fit the object to our data. That is, run a correlation test for every X variable with our y variable, and then rank the X variables based on how correlated they are with the y/target variable. Then give me the top 10 features.\n",
    "3. Use get_support() to get the list of features, and save them to a variable that you can use to filter your dataframe in modeling.\n",
    "\n",
    "\n",
    "\n",
    "1. make the thing\n",
    "2. fit the thing\n",
    "3. use the thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d75cbae94f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# find the top 8 X's correlated with y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# boolean mask of whether the column was selected or not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# parameters: f_regression stats test, give me 8 features\n",
    "f_selector = SelectKBest(f_regression, k=8)\n",
    "    # send in f_regression\n",
    "    # k is the number of features we want the best of\n",
    "        # top 8 best in this case\n",
    "    # figuring out your k may take some trial and error\n",
    "\n",
    "# find the top 8 X's correlated with y\n",
    "f_selector.fit(X_train_scaled, y_train)\n",
    "    # \n",
    "\n",
    "# boolean mask of whether the column was selected or not. \n",
    "feature_mask = f_selector.get_support()\n",
    "    # get support gives us back a feture mask\n",
    "        # this one is an arry of booleans\n",
    "            # basically is X_train_scaled.columns.tolist()\n",
    "            # filters the columns\n",
    "            \n",
    "\n",
    "# get list of top K features. \n",
    "f_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean mask is a name for an array of cooleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top featues # is indicated by k in f_selector\n",
    "    # if you set k to 3 you will get 3\n",
    "    # if set to 4 you will get the tip 4\n",
    "    # etc.\n",
    "f_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **when we model on features we are modeling on their interplay of each other**\n",
    "    - if use `k=1 `\n",
    "        - we get G2 (midterm)\n",
    "    - `k=2`\n",
    "        - G1 and G2 (midterm)\n",
    "    - `k=3`\n",
    "        - failures, G1, and G2\n",
    "    - `k=4`\n",
    "        - medu, fedi, failure, G1, G2\n",
    "    - `k=5`\n",
    "        - age, medu, fedu, failure, G1, G2\n",
    "    - `k=6`\n",
    "        - age, medu, fedu, traveltime, failure, G1, G2\n",
    "    - `k=7`\n",
    "        - age, medu, traveltime, studytime, failure, G1, G2\n",
    "            - as k is growing we are modeling on the interplay of tall the features\n",
    "    - `k=14`\n",
    "        - \n",
    "            - as we tack on more features the importantce of a feature can also change\n",
    "- there isnt really a magic k number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Recursive Feature Elimination will create a model with all the features, evaluate the performance metrics, find the weakest feature, remove it, then create a new model with the remaining features, evaluate the performance metrics, find the weakest feature, remove it, and so on, until it gets down to the number of features you have indicated you want when creating the rfe object. You will also indicate which Machine Learning algorithm you want to use.\n",
    "\n",
    "1. Initialize the machine learning algorithm, in this case LinearRegression\n",
    "2. Initialize the RFE object, and provide the ML algorithm object from step 1\n",
    "3. Fit the RFE object to our data. Doing this will provide us a list of features (the number we asked for) as well as a ranking of all the features.\n",
    "4. Assign the list of selected features to a variable.\n",
    "5. Optional: Get ranking of all variables (1 being most important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# hey look it is pretty linear\n",
    "    # it is not perfect\n",
    "    # but now I feel better about using a linear model\n",
    "        # so the lesson is to not just auto use rfe \n",
    "            # unless there is a linear relationship between the feature(s) and the targe variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE \n",
    "\n",
    "# initialize the ML algorithm\n",
    "    # in this case LinearRegression\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create the rfe object, indicating the ML object (lm) and the number of features I want to end up with. \n",
    "    # Initialize the RFE object, and provide the ML algorithm\n",
    "rfe = RFE(lm, 3)\n",
    "    # the 3 is how many features you want to end up with\n",
    "    # may take some trial and error\n",
    "    # is up to you\n",
    "\n",
    "# fit the data using RFE\n",
    "rfe.fit(X_train_scaled,y_train)\n",
    "    # fit on scaled data\n",
    "\n",
    "# get the mask of the columns selected\n",
    "feature_mask = rfe.support_\n",
    "    # boolean mask\n",
    "        # a bunch of trues and falses\n",
    "\n",
    "# get list of the column names. \n",
    "rfe_feature = X_train_scaled.iloc[:,feature_mask].columns.tolist()\n",
    "    # pair down the list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_feature\n",
    "# importance is read like:\n",
    "    # least important -----> importnat -----> most important\n",
    " # if features are cranked up we may get a different importance set up than when we run it with 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view list of columns and their ranking\n",
    "\n",
    "# get the ranks\n",
    "var_ranks = rfe.ranking_\n",
    "    #gives us the rank\n",
    "    # anything that is coming off a model with an \"_\"\n",
    "\n",
    "# get the variable names\n",
    "var_names = X_train_scaled.columns.tolist()\n",
    "\n",
    "# combine ranks and names into a df for clean viewing\n",
    "rfe_ranks_df = pd.DataFrame({'Var': var_names, 'Rank': var_ranks})\n",
    "\n",
    "# sort the df by rank\n",
    "rfe_ranks_df.sort_values('Rank')\n",
    "    # sort your values\n",
    "    \n",
    "# get your happy littld dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is not a magic number for the right number of features to end up with\n",
    "    # pros and cons to both manyb and little amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Longer Lesson\n",
    "This part of the lesson goes through what we just covered above, but with more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select K Best\n",
    "The goal of filter methods, such as SelectKBest, is to keep the attributes with highest correlation to the target variable and of those features, if two are highly correlated with each other, remove one of them. With filter methods, the model is built after selecting the features. These methods identify the relevant features and subset the data with only those features.\n",
    "\n",
    "Select K Best is a filter method, meaning the goal is to find and keep the attributes with highest correlation to the target variable and of those features, if two are highly correlated with each other, remove one of them.\n",
    "\n",
    "`SelectKBest` will identify the K most relevant features and subset the data with only those features. Relevancy is determined by the the test statistic for the chosen function or test (Chi-squared, F-regression, etc.). For regression, we will use the f-regression test to score the individual effect of each of the features (aka regressors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the f_selector object, setting the parameters, or instructions for the method to follow: \"use the f_regression test for scoring the features, and return to me the top 10 features\", for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector = SelectKBest(f_regression, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the object to our data. In doing this, our selector is scoring, ranking, and identifying the top k features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_selector.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform our dataset to reduce to the k best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = f_selector.transform(X_train_scaled)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify steps 1-3 in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced2 = SelectKBest(f_regression, k=2).fit_transform(X_train_scaled, y_train)\n",
    "print(X_reduced2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the inverse_transform function to return to the original variables.\n",
    "\n",
    "Let's say we want a list of the features we have selected. Why? Maybe we want to run various feature selection methods and want to keep track of how many times each feature was selected. We could simply grab the column names from our new dataframe above that contains only the best 2 features. However, maybe we don't need the new dataframe yet, as we aren't quite sure which features we will finally decide to keep. In that case, we would create the object using SelectKBest, fit it to the data (not transform), and then we could use get_support to get a mask, or integer index, of the features the algorithm selected.\n",
    "\n",
    "Let's walk through that. You will see below we end up with a list of booleans that relate to the feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_support = f_selector.get_support()\n",
    "\n",
    "print(f_support) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a list of the feature names selected from X_train using .loc with our mask, using .columns to get the column names, and convert the values to a list using .tolist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_feature = X_train_scaled.loc[:,f_support].columns.tolist()\n",
    "\n",
    "# you could also get the list this way (among many others)\n",
    "# f_feature = [X_train_scaled.columns.values[i] for i in range(len(feature_mask)) if feature_mask[i]==True]\n",
    "\n",
    "print(str(len(f_feature)), 'selected features')\n",
    "print(f_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we used the SelectKBest method to select the top k features, and these features are scored and ranked using a statistical test, which we used the f-regression test in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination\n",
    "Recursive Feature Elimination is a wrapper method for feature selection. This means that it works by using the output of a machine learning algorithm as the evaluation criteria for eliminating features; in the case of linear regression, it uses the resulting coefficients.\n",
    "\n",
    "You feed all the features to the selected Machine Learning algorithm, and, based on the the hyperparameters you have set, features are removed. One word of caution...this is an iterative and computationally expensive process! The pro is that it is more accurate than SelectKBest.\n",
    "\n",
    "RFE recursively removes attributes and then builds a model on those attributes that remain. The RFE method takes the machine learning algorithm to be used and the number of required features as input. It returns the ranking of all the variables, 1 being most important, along with its support: a list of boolean values, True indicating relevant features and False indicating irrelevant features.\n",
    "\n",
    "These are the steps we will take to implement RFE:\n",
    "\n",
    "Initialize the linear regression object. sklearn.linear_model.LinearRegression\n",
    "Initialize the RFE object. sklearn.feature_selection.RFE\n",
    "Fit the RFE object to our data. rfe.fit()\n",
    "Transform our X dataframe to include only those 2 features. rfe.transform()\n",
    "Optional: Get list of features selected\n",
    "Optional: Get ranking of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the linear regression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the RFE object, setting the hyperparameters to be our linear regression object created above (as the algorithm to test the features on) and the number of features to return to be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(lm, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the RFE object to our data. This means create multiple linear regression models, find the one that performs best, and identify the features that are used in that model. Those are the features we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform our X dataframe to include only those 2 features. .transform() or do both of those steps together with .fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we move on to modeling, we would then use our new X dataframe as the one to move forward for actual modeling. As a sneak peak..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the data to model\n",
    "lm.fit(X_rfe,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a list of the features that remain, we can use .support_ similar to how we used .get_support() with SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_features = X_train_scaled.loc[:,mask].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(rfe_features)), 'selected features')\n",
    "print(rfe_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a ranking of the features using rfe.ranking_. This will return a 1 for the features that were selected. So, since we said we wanted 2 features to remain, the top two features will have a rank of 1. The features that were eliminated will be ranked accordingly. In this case, the third feature will have a rank of 2. However, if we had more than 1 feature that was eliminated, they would all have different ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ranks = rfe.ranking_\n",
    "var_names = X_train_scaled.columns.tolist()\n",
    "\n",
    "pd.DataFrame({'Var': var_names, 'Rank': var_ranks})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we took LinearRegression model with 2 features and RFE gave feature ranking as above, but the selection of number ‘2’ was random. If you would like to learn how to find the optimum number of features, for which the accuracy is the highest, see the extended lesson in the appendix of ds.codeup.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "We used SelectKBest to select the top 2 features based on how correlated each feature is with the target variable. We ended up with exam1 and exam3.\n",
    "We use RFE and a linear regression algorithm to keep the top 2 features based on which features lead to the best performing linear regression model. This eliminated exam2 and also left us with exam1 and exam3, like SelectKBest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
